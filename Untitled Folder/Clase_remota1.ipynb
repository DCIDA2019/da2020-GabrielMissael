{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase remota 1 del día 27/03/2020\n",
    "\n",
    "No siempre el conjunto de parametros que mejor se ajusta a mis datos es el que es mas probable para mi modelo, es decir, el mejor ajuste podria tener parametros que son casi imposibles.\n",
    "La respuesta a ambas preguntas (paramtreos con mejor ajuste o mas probables) muchas veces es la misma, pero esto no siempre es asi.\n",
    "\n",
    "Para encontrar los valores mas probables, puedo tomar el conjunto de valores que mas se aproximan conjuntamente al minimo de manera recurrente en mi minimización. \n",
    "\n",
    "### Likelihood\n",
    "Es una cosntrucción que realizamos para establecer la probabilidad de que nuestra hipotesis sobre un modelo o teoriea sea cierta dada que estamos observando unos datos determinados. Es la probabilidad de que el modelo sea el que describa a los datos.\n",
    "\n",
    "No tiene una forma unica, depende del tipo de datos que estoy manejando, es decir, que tipo de distribución de probabilidad tienen estos. Por ejemplo, si los datos son gaussianos, entonces podemos pensar que el likelihood también es gaussiano. Tenemos, en general que:\n",
    "\n",
    "$ L \\propto \\prod \\limits_{i}^{n} \\frac{1}{2\\pi\\sigma_i^2}exp(-\\frac{(y_i-\\mu)^2}{2\\sigma_i^2})$\n",
    "\n",
    "Por lo  general una minimización de chi-square corresponde a una maximización de el likelihood.Es importante notar que la preferencia entre minimizar chi-square o maximizar el likelihood (cuando estos no ocurren paraleleamente) depende completamente del problema que estemos abordando. No hay algo como uno mejor que otro.\n",
    "\n",
    "Por ejemplo, en terminos de puntos de datos y paramatros (donde lambda es nuestro modelo para cada $y_i$) asumiendo que los datos tienen una **distribución gaussiana** de los datos, entonces el likilehood se puede obtener cómo:\n",
    "\n",
    "$-ln(L(\\vec x,\\vec y|\\vec\\theta) \\propto \\frac{1}{2} \\sum_i (\\frac{(y_i-\\lambda(x_i,\\vec\\theta))^2}{\\sigma_i^2})$\n",
    "\n",
    "Es util expresarlo de esta manera para eliminar el exponencial y tener datos mas 'digeribles'. Pero ¿que pasa si queremos maximizar el likelihood de muchos parametros para un modelo bastante complejo y si ademas no es gaussiano? Para ello nos sirve el metodo montecarlo.\n",
    "\n",
    "### MonteCarlo - cadenas de Markov\n",
    "\n",
    "Este metodo consiste en generar un muestreo aleatorio para los parametros (por ejemplo, en la ecuación anterior, las $\\vec\\theta$ son los parametros libres que podemos muestrear), y despues los rechaza o los acepta dependiendo del likelihood que estos datos tengan. Esta discriminación se hace de acuerdo a likelihood anteriores, y en general, el algoritmo es:\n",
    "\n",
    "1. Elige un punto de partida $\\vec p_{old}$.\n",
    "2. Genera un punto nuevo en la vecindad de $\\vec p_{old}$, dado una distribución gaussiana:\n",
    "    - $p_{new}i = N(p_{old}i, \\sigma_i)$\n",
    "    \n",
    "   Es importante notar que cada parametro puede     tener un sigma distinto\n",
    "3. Si el likelihood de la nueva muestra es mas alta que la anterior, guardamos el nuevo:\n",
    "    - si $L(\\vec p_{new})>L(\\vec p_{old})$ entonces:\n",
    "    - $\\vec p_{old} = p_{new}$\n",
    "4. Si el likelihood de la nueva muestra es menor a la anterior, entonces tomamos un numero aleatorio $U = N(0,1)$; si el cociente de los likelihood $C = (p_{new})/(p_{old})$ es mas grande que dicho numero aleatorio, entonces la aceptamos.\n",
    "5. Si no cumple ninguna de las dos, entonces lo rechazamos.\n",
    "6. Regresa al paso 2, hasta dar N pasos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despues de haber realizado los N pasos, vemos la distribición resultante (las cadenas) de los parametros, y por lo tanto, su likelihood :).\n",
    "\n",
    "Ojo: Checar el periodo de 'quemado' y la convergencia. \n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Bernardita_Ried_Guachalla/publication/335291562/figure/fig2/AS:794195358924800@1566362337030/Example-of-a-Markov-Chain-Monte-Carlo-for-a-Multivariate-Gaussian-Distribution.ppm\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Por lo general, teniendo un numero dado de parametros y despues de haber realizado el metodo montecarlo, los resulatados se suelen representar de la siguiente manera (muy util, y en 2d):\n",
    "\n",
    "<img src=\"https://inspirehep.net/record/1720816/files/CMB+BAO+JLA_Al_usual_thesis.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Esta representación nos ayuda para observar la dependencia o la 'interacción' entre los parametros en el resultado obtenido.\n",
    "\n",
    "## Algoritmo en Python\n",
    "\n",
    "A continuación, un algoritmo general de este metodo de montecarlo :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
